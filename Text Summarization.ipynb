{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e383de7d",
   "metadata": {},
   "source": [
    "## goal:\n",
    "\n",
    "Create a web application that takes news stories from specific sites and summarizes each article and categorizes article each into specific categories, presenting a summary and category to a user on the screen\n",
    "\n",
    "Need to do:\n",
    "\n",
    "1. **Web Scraping:**\n",
    "   Use a Python library like Beautiful Soup and Requests or a specialized library like Scrapy to scrape news articles from specific websites. You'll need to identify the HTML structure of the articles on these sites and extract relevant information, such as the article content, title, and category.\n",
    "\n",
    "2. **Text Summarization:**\n",
    "   Implement text summarization using a library like Gensim, NLTK, or Hugging Face Transformers (for advanced models like BERT). The summarization process should take the full article text and produce a shorter summary.\n",
    "\n",
    "3. **Text Classification:**\n",
    "   For categorization, you'll need a machine learning model for text classification. You can train your own model using a dataset of categorized articles, or you can use pre-trained models like those from the Hugging Face Transformers library. The model will take the article text and categorize it into specific categories.\n",
    "\n",
    "4. **Web Framework:**\n",
    "   Choose a web framework for building your web application. Flask and Django are popular options in Python. Set up your application with routes for user interaction and displaying results.\n",
    "\n",
    "5. **Database (Optional):**\n",
    "   You can store scraped articles, summaries, and categories in a database for better management and retrieval.\n",
    "\n",
    "6. **User Interface:**\n",
    "   Design and develop the user interface to present the summarized articles and their categories to users. You can use HTML, CSS, and JavaScript for the front end. Libraries like Bootstrap can help with styling.\n",
    "\n",
    "7. **Integration:**\n",
    "   Integrate the web scraping, text summarization, and text classification components into your web application. When a user requests news, the application should scrape articles, summarize them, and classify them in real-time.\n",
    "\n",
    "8. **User Interaction:**\n",
    "   Implement user interaction features to allow users to request news from specific sites, view article summaries, and see categorized results.\n",
    "\n",
    "9. **Deployment:**\n",
    "   Deploy your web application on a server. You can use platforms like Heroku, AWS, or a VPS to host your application and make it accessible on the web.\n",
    "\n",
    "10. **Testing and Maintenance:**\n",
    "    Thoroughly test your web application to ensure it works as expected. Monitor for issues and maintain the application over time.\n",
    "\n",
    "based on this, this notebook should have the web scraping, storing scraped data into a database, and the model to generate text summaries. Django part to be completed later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7b8dac",
   "metadata": {},
   "source": [
    "Here's a list of free news websites that provide global news:\n",
    "\n",
    "1. **BBC News** - [https://www.bbc.com/news](https://www.bbc.com/news)\n",
    "\n",
    "2. **CNN** - [https://www.cnn.com/](https://www.cnn.com/)\n",
    "\n",
    "3. **Al Jazeera** - [https://www.aljazeera.com/](https://www.aljazeera.com/)\n",
    "\n",
    "4. **Reuters** - [https://www.reuters.com/](https://www.reuters.com/)\n",
    "\n",
    "5. **NPR** - [https://www.npr.org/](https://www.npr.org/)\n",
    "\n",
    "6. **The Guardian** - [https://www.theguardian.com/](https://www.theguardian.com/)\n",
    "\n",
    "7. **The New York Times** - [https://www.nytimes.com/](https://www.nytimes.com/)\n",
    "\n",
    "8. **BBC World Service** - [https://www.bbc.co.uk/worldserviceradio](https://www.bbc.co.uk/worldserviceradio)\n",
    "\n",
    "9. **Bloomberg** - [https://www.bloomberg.com/](https://www.bloomberg.com/)\n",
    "\n",
    "10. **AP News** - [https://apnews.com/](https://apnews.com/)\n",
    "\n",
    "These websites provide free access to a wide range of news articles covering global events and topics. Please note that the availability of content and access may vary by region, and some websites may offer premium content or subscription options alongside their free offerings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8227337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abc616e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f030cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb0fe34d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\jessi\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: newspaper3k in c:\\users\\jessi\\anaconda3\\lib\\site-packages (0.2.8)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from newspaper3k) (4.12.2)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from newspaper3k) (10.0.1)\n",
      "Requirement already satisfied: PyYAML>=3.11 in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from newspaper3k) (6.0)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from newspaper3k) (1.2.0)\n",
      "Requirement already satisfied: lxml>=3.6.0 in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from newspaper3k) (4.9.3)\n",
      "Requirement already satisfied: nltk>=3.2.1 in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from newspaper3k) (3.8.1)\n",
      "Requirement already satisfied: requests>=2.10.0 in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from newspaper3k) (2.31.0)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from newspaper3k) (6.0.10)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from newspaper3k) (5.0.1)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from newspaper3k) (0.0.4)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from newspaper3k) (0.35.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from newspaper3k) (2.8.2)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from newspaper3k) (0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.5)\n",
      "Requirement already satisfied: six in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
      "Requirement already satisfied: click in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (4.65.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2023.7.22)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (3.12.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from click->nltk>=3.2.1->newspaper3k) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk\n",
    "# !pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "020acb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement unicodedata (from versions: none)\n",
      "ERROR: No matching distribution found for unicodedata\n"
     ]
    }
   ],
   "source": [
    "#!pip install pysummarization\n",
    "#!pip install contractions\n",
    "#!pip install unicodedata\n",
    "#!pip install regex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c008c8",
   "metadata": {},
   "source": [
    "#### Getting the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b657b40d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.bbc.com/news/live/world-middle-east-67831997\n",
      "https://www.bbc.com/news/world-middle-east-67831478\n",
      "https://www.bbc.com/news/world-europe-67827443\n",
      "https://www.bbc.com/news/world-us-canada-67833339\n",
      "https://www.bbc.com/news/entertainment-arts-67832513\n",
      "https://www.bbc.com/news/science-environment-67718719\n",
      "https://www.bbc.com/news/world-europe-67826487\n",
      "https://www.bbc.com/news/world-us-canada-67830918\n",
      "https://www.bbc.com/news/world-us-canada-67832595\n",
      "https://www.bbc.com/news/world-us-canada-67835961\n",
      "https://www.bbc.com/news/world-us-canada-67832651\n",
      "https://www.bbc.com/news/world-us-canada-67835961\n",
      "https://www.bbc.com/news/world-us-canada-67832651\n",
      "https://www.bbc.com/news/world-latin-america-67826945\n",
      "https://www.bbc.com/news/world-us-canada-67829682\n",
      "https://www.bbc.com/news/world-us-canada-67834817\n",
      "https://www.bbc.com/news/world-latin-america-66786995\n",
      "https://www.bbc.com/news/world_radio_and_tv\n",
      "https://www.bbc.com/sounds/play/live:bbc_world_service\n",
      "https://www.bbc.com/news/world-asia-67770782\n",
      "https://www.bbc.com/news/in-pictures-67542176\n",
      "https://www.bbc.com/news/world-us-canada-67808305\n",
      "https://www.bbc.com/news/entertainment-arts-67413429\n",
      "https://www.bbc.com/news/world-asia-india-67759073\n",
      "https://www.bbc.com/news/world-us-canada-67832595\n",
      "https://www.bbc.com/news/world-65432059\n",
      "https://www.bbc.com/news/uk-67830592\n",
      "https://www.bbc.com/news/uk-scotland-67829546\n",
      "https://www.bbc.com/news/world-67465190\n",
      "https://www.bbc.com/news/uk-67773394\n",
      "https://www.bbc.com/news/world-us-canada-67808808\n",
      "https://www.bbc.com/news/uk-england-hampshire-67628350\n",
      "https://www.bbc.com/news/business-67828891\n",
      "https://www.bbc.com/news/world-europe-67830858\n",
      "https://www.bbc.com/news/technology-67826601\n",
      "https://www.bbc.com/news/world-europe-67832413\n",
      "https://www.bbc.com/news/world-europe-67827443\n",
      "https://www.bbc.com/news/world-us-canada-67833339\n",
      "https://www.bbc.com/news/entertainment-arts-67832513\n",
      "https://www.bbc.com/news/world-latin-america-66786995\n",
      "https://www.bbc.com/news/uk-england-hampshire-67628350\n",
      "https://www.bbc.com/news/science-environment-67718719\n",
      "https://www.bbc.com/news/world-asia-67770782\n",
      "https://www.bbc.com/news/uk-67773394\n",
      "https://www.bbc.com/news/world-europe-67826487\n",
      "https://www.bbc.com/news/world-us-canada-67834817\n",
      "https://www.bbc.com/future/article/20231228-2023-an-explosive-year-in-space-exploration\n",
      "https://www.bbc.com/worklife/article/20231219-ten-major-events-that-shaped-business-in-2023\n",
      "https://www.bbc.com/travel/article/20231228-frikadeller-the-classic-danish-comfort-food-dish\n",
      "https://www.bbc.com/culture/article/20231220-why-the-flying-scotsman-is-a-symbol-of-britishness\n",
      "https://www.bbc.com/future/article/20231222-how-humans-have-changed-earths-surface-in-2023\n",
      "https://www.bbc.com/worklife/article/20231219-panic-and-possibility-what-workers-learned-about-ai-in-2023\n",
      "https://www.bbc.com/travel/article/20231227-five-cities-where-your-travel-cash-goes-further\n",
      "https://www.bbc.com/sport/live/football/67255960\n",
      "https://www.bbc.com/sport/cricket/67835752\n",
      "https://www.bbc.com/sport/rugby-union/67836548\n",
      "https://www.bbc.com/sport/cricket/67831929\n",
      "https://www.bbc.com/sport/tennis/67760747\n",
      "https://www.bbc.com/sport/basketball/67831931\n",
      "https://www.bbc.com/sport/africa/67631299\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "\n",
    "url = \"https://www.bbc.co.uk/news\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request is successful\n",
    "if response.status_code == 200:\n",
    "    # get content\n",
    "    soup = bs(response.text, \"html.parser\")\n",
    "\n",
    "    # find all anchor elements with the \"gs-c-promo-heading\" class\n",
    "    article_links = soup.find_all(\"a\", class_=\"gs-c-promo-heading\")\n",
    "\n",
    "    # Create list to store the article URLs\n",
    "    article_urls = []\n",
    "\n",
    "    # Extract article URLs from the href attribute and add to the list\n",
    "    for link in article_links:\n",
    "        article_url = \"https://www.bbc.com\" + link.get(\"href\")\n",
    "        article_urls.append(article_url)\n",
    "\n",
    "    # Clean up URLs with duplicate \"https://www.bbc.com\"\n",
    "    cleaned_urls = []\n",
    "    for url in article_urls:\n",
    "        # Check if \"https\" appears twice in the URL\n",
    "        if url.count(\"https\") == 2:\n",
    "            # Remove the first occurrence of \"https://www.bbc.com\"\n",
    "            cleaned_url = url.replace(\"https://www.bbc.com\", \"\", 1)\n",
    "            cleaned_urls.append(cleaned_url)\n",
    "        else:\n",
    "            cleaned_urls.append(url)\n",
    "\n",
    "    # Print the cleaned list of article URLs\n",
    "    for url in cleaned_urls:\n",
    "        print(url)\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve the page.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d1aed0",
   "metadata": {},
   "source": [
    "#### Extracting Article Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "913cd83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to extract information from a URL\n",
    "def extract_info(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "\n",
    "    # Extracted information\n",
    "    title = article.title\n",
    "    text = article.text\n",
    "    authors = article.authors if article.authors else [\"Author not found\"]\n",
    "\n",
    "    # Generate summary\n",
    "    article.nlp()\n",
    "    summary = article.summary\n",
    "\n",
    "    return {\n",
    "        \"URL\": url,\n",
    "        \"Title\": title,\n",
    "        \"Authors\": authors,\n",
    "        \"Text\": text,\n",
    "        \"TF_IDF_Summary\": summary\n",
    "    }\n",
    "\n",
    "# Extract information for each cleaned URL in the list\n",
    "results = [extract_info(url) for url in cleaned_urls]\n",
    "\n",
    "# Create a Pandas DataFrame from the results\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Get the current date in the format YYYY-MM-DD\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# change the path to save the files for organization\n",
    "directory_path = r\"C:\\Users\\jessi\\Desktop\\Projects Personal\\bbc_article_extracts\"\n",
    "\n",
    "# Combine the directory path with the CSV filename\n",
    "csv_filename = f\"{directory_path}\\\\bbc_articles_info_{current_date}.csv\"\n",
    "\n",
    "# Export the DataFrame to a CSV file with the updated path\n",
    "df.to_csv(csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f061bdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, the summaries created using the newspaper package use the tf-idf method for summarization\n",
    "# Will try to use the BERT model for summaries\n",
    "\n",
    "# note for this, look at huggingface transformers library, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89a81088",
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE THIS TO CHECK DF IN EXCEL\n",
    "# Get the current date in the format YYYY-MM-DD\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Export the DataFrame to a CSV file with the date in the name\n",
    "csv_filename = f\"bbc_articles_info_testing_{current_date}.csv\"\n",
    "df.to_csv(csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b427537",
   "metadata": {},
   "source": [
    "#### Cleaning the Data/Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c0a368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding contractions\n",
    "\n",
    "import contractions\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "# Function to expand contractions in a given text\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "# Apply the function to the 'Text' column and create a new column for cleaned text\n",
    "df['clean_article_text'] = df['Text'].apply(expand_contractions)\n",
    "\n",
    "\n",
    "# Removing unicode characters\n",
    "def remove_unicode(text):\n",
    "    return ''.join(char for char in unicodedata.normalize('NFD', text) if unicodedata.category(char) != 'Mn')\n",
    "\n",
    "# Apply this to the column we just made (will be keeping clean text/updates in one column)\n",
    "df['clean_article_text'] = df['clean_article_text'].apply(remove_unicode)\n",
    "\n",
    "\n",
    "# Converting to lower case\n",
    "df['clean_article_text'] = df['clean_article_text'].str.lower()\n",
    "\n",
    "# Removing special characters and punctuation\n",
    "def remove_special_characters(text):\n",
    "    # Using regex to remove non-alphanumeric characters\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "# Apply the function to the 'clean_article_text' column and update it\n",
    "df['clean_article_text'] = df['clean_article_text'].apply(remove_special_characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac7dae7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jessi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jessi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#### testing this\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download punkt tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Download NLTK stop words data\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to convert list to string, tokenize, and remove stop words\n",
    "def preprocess_text(text_list):\n",
    "    text_string = ' '.join(text_list)\n",
    "    tokens = word_tokenize(text_string)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Convert 'clean_article_text' column from list to string and apply preprocessing\n",
    "df['clean_article_text'] = df['clean_article_text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76db52ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jessi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jessi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(filtered_words)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Apply stop words removal to column in df\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_article_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_article_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(remove_stopwords)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4760\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4627\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4632\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4633\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4634\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4635\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4636\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4751\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4752\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4753\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4754\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4755\u001b[0m         func,\n\u001b[0;32m   4756\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4757\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4758\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4759\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4760\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1207\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1206\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1287\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1281\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1287\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1288\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1289\u001b[0m )\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2917\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[4], line 23\u001b[0m, in \u001b[0;36mremove_stopwords\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_stopwords\u001b[39m(text):\n\u001b[0;32m     22\u001b[0m     stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m---> 23\u001b[0m     words \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(text)\n\u001b[0;32m     24\u001b[0m     filtered_words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(filtered_words)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1281\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   1278\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentences_from_text(text, realign_boundaries))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1341\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\n\u001b[0;32m   1333\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1334\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1341\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\n\u001b[0;32m   1333\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1334\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1329\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m realign_boundaries:\n\u001b[0;32m   1328\u001b[0m     slices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[1;32m-> 1329\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m slices:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (sentence\u001b[38;5;241m.\u001b[39mstart, sentence\u001b[38;5;241m.\u001b[39mstop)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1459\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;124;03mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[0;32m   1448\u001b[0m \u001b[38;5;124;03mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;124;03m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[0;32m   1457\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1458\u001b[0m realign \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1459\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence1, sentence2 \u001b[38;5;129;01min\u001b[39;00m _pair_iter(slices):\n\u001b[0;32m   1460\u001b[0m     sentence1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(sentence1\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m realign, sentence1\u001b[38;5;241m.\u001b[39mstop)\n\u001b[0;32m   1461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sentence2:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:321\u001b[0m, in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    319\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(iterator)\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m     prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1431\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_slices_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mslice\u001b[39m]:\n\u001b[0;32m   1430\u001b[0m     last_break \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1431\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m match, context \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_potential_end_contexts(text):\n\u001b[0;32m   1432\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[0;32m   1433\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(last_break, match\u001b[38;5;241m.\u001b[39mend())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1393\u001b[0m previous_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   1394\u001b[0m previous_match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang_vars\u001b[38;5;241m.\u001b[39mperiod_context_re()\u001b[38;5;241m.\u001b[39mfinditer(text):\n\u001b[0;32m   1396\u001b[0m \n\u001b[0;32m   1397\u001b[0m     \u001b[38;5;66;03m# Get the slice of the previous word\u001b[39;00m\n\u001b[0;32m   1398\u001b[0m     before_text \u001b[38;5;241m=\u001b[39m text[previous_slice\u001b[38;5;241m.\u001b[39mstop : match\u001b[38;5;241m.\u001b[39mstart()]\n\u001b[0;32m   1399\u001b[0m     index_after_last_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_last_whitespace_index(before_text)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object, got 'list'"
     ]
    }
   ],
   "source": [
    "# using NLTK to do further cleaning\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "\n",
    "# Download punkt tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Apply tokenization to column in df\n",
    "df['clean_article_text'] = df['clean_article_text'].apply(word_tokenize)\n",
    "\n",
    "# Stop word removal\n",
    "\n",
    "# Download NLTK stop words data\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to remove stop words\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = nltk.word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply stop words removal to column in df\n",
    "df['clean_article_text'] = df['clean_article_text'].apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244c58a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Using Pysummarization\n",
    "# refer to the documentation online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f1939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Using BERT via huggingface transformers (or use T5) check the link\n",
    "#  https://keras.io/examples/nlp/t5_hf_summarization/       this uses T5\n",
    "\n",
    "# https://datagraphi.com/blog/post/2021/9/24/comparing-performance-of-a-modern-nlp-framework-bert-vs-a-classical-approach-tf-idf-for-document-classification-with-simple-and-easy-to-understand-code\n",
    "# ^ this uses BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a220660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### for bert\n",
    "\n",
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e558d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(batch['article'], padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "train_dataset = train_dataset.map(tokenize_batch, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_batch, batched=True)\n",
    "\n",
    "import torch\n",
    "from transformers import BertForSeq2Seq\n",
    "\n",
    "model = BertForSeq2Seq.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    # Define your evaluation metric (e.g., BLEU, ROUGE, etc.)\n",
    "    # For this example, we'll use dummy metrics\n",
    "    return {\"accuracy\": 0.5}\n",
    "\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=500,\n",
    "    logging_dir='./logs',\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=1e-5,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]), \n",
    "                                'attention_mask': torch.stack([f[1] for f in data]), \n",
    "                                'labels': torch.stack([f[2] for f in data])},\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e739be9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## bert -> prolly drop this part, above is using cnn dataset to train model and eval.\n",
    "# from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "# import torch\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertForConditionalGeneration.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Extract cleaned text data from the dataframe\n",
    "# texts = df['cleaned_article_text'].tolist()\n",
    "\n",
    "# # Tokenize the texts\n",
    "# inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# # Generate summaries\n",
    "# outputs = model.generate(inputs.input_ids, max_length=150, num_beams=2, early_stopping=True)\n",
    "\n",
    "# # Decode the generated summaries\n",
    "# generated_summaries = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "# # store the summaries\n",
    "# df['Bert_generated_summary'] = generated_summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d362d4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## bert after trained/tuned on my own data\n",
    "# Tokenizing the dataframe\n",
    "def tokenize_articles(text):\n",
    "    inputs = tokenizer(text, max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    return inputs\n",
    "\n",
    "df['tokenized'] = df['cleaned_article_text'].apply(tokenize_articles)\n",
    "\n",
    "def generate_summary(input_ids, attention_mask):\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    \n",
    "    # Generate summary\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=150, num_beams=4, length_penalty=2.0, early_stopping=True)\n",
    "    \n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "df['generated_summary'] = df['tokenized'].apply(lambda x: generate_summary(x['input_ids'], x['attention_mask']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20304b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
