{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e383de7d",
   "metadata": {},
   "source": [
    "## goal:\n",
    "\n",
    "Create a web application that takes news stories from specific sites and summarizes each article and categorizes article each into specific categories, presenting a summary and category to a user on the screen\n",
    "\n",
    "Need to do:\n",
    "\n",
    "1. **Web Scraping:**\n",
    "   Use a Python library like Beautiful Soup and Requests or a specialized library like Scrapy to scrape news articles from specific websites. You'll need to identify the HTML structure of the articles on these sites and extract relevant information, such as the article content, title, and category.\n",
    "\n",
    "2. **Text Summarization:**\n",
    "   Implement text summarization using a library like Gensim, NLTK, or Hugging Face Transformers (for advanced models like BERT). The summarization process should take the full article text and produce a shorter summary.\n",
    "\n",
    "3. **Text Classification:**\n",
    "   For categorization, you'll need a machine learning model for text classification. You can train your own model using a dataset of categorized articles, or you can use pre-trained models like those from the Hugging Face Transformers library. The model will take the article text and categorize it into specific categories.\n",
    "\n",
    "4. **Web Framework:**\n",
    "   Choose a web framework for building your web application. Flask and Django are popular options in Python. Set up your application with routes for user interaction and displaying results.\n",
    "\n",
    "5. **Database (Optional):**\n",
    "   You can store scraped articles, summaries, and categories in a database for better management and retrieval.\n",
    "\n",
    "6. **User Interface:**\n",
    "   Design and develop the user interface to present the summarized articles and their categories to users. You can use HTML, CSS, and JavaScript for the front end. Libraries like Bootstrap can help with styling.\n",
    "\n",
    "7. **Integration:**\n",
    "   Integrate the web scraping, text summarization, and text classification components into your web application. When a user requests news, the application should scrape articles, summarize them, and classify them in real-time.\n",
    "\n",
    "8. **User Interaction:**\n",
    "   Implement user interaction features to allow users to request news from specific sites, view article summaries, and see categorized results.\n",
    "\n",
    "9. **Deployment:**\n",
    "   Deploy your web application on a server. You can use platforms like Heroku, AWS, or a VPS to host your application and make it accessible on the web.\n",
    "\n",
    "10. **Testing and Maintenance:**\n",
    "    Thoroughly test your web application to ensure it works as expected. Monitor for issues and maintain the application over time.\n",
    "\n",
    "based on this, this notebook should have the web scraping, storing scraped data into a database, and the model to generate text summaries. Django part to be completed later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7b8dac",
   "metadata": {},
   "source": [
    "Here's a list of free news websites that provide global news:\n",
    "\n",
    "1. **BBC News** - [https://www.bbc.com/news](https://www.bbc.com/news)\n",
    "\n",
    "2. **CNN** - [https://www.cnn.com/](https://www.cnn.com/)\n",
    "\n",
    "3. **Al Jazeera** - [https://www.aljazeera.com/](https://www.aljazeera.com/)\n",
    "\n",
    "4. **Reuters** - [https://www.reuters.com/](https://www.reuters.com/)\n",
    "\n",
    "5. **NPR** - [https://www.npr.org/](https://www.npr.org/)\n",
    "\n",
    "6. **The Guardian** - [https://www.theguardian.com/](https://www.theguardian.com/)\n",
    "\n",
    "7. **The New York Times** - [https://www.nytimes.com/](https://www.nytimes.com/)\n",
    "\n",
    "8. **BBC World Service** - [https://www.bbc.co.uk/worldserviceradio](https://www.bbc.co.uk/worldserviceradio)\n",
    "\n",
    "9. **Bloomberg** - [https://www.bloomberg.com/](https://www.bloomberg.com/)\n",
    "\n",
    "10. **AP News** - [https://apnews.com/](https://apnews.com/)\n",
    "\n",
    "These websites provide free access to a wide range of news articles covering global events and topics. Please note that the availability of content and access may vary by region, and some websites may offer premium content or subscription options alongside their free offerings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8227337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abc616e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f030cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0fe34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c46fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020acb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pysummarization\n",
    "#!pip install contractions\n",
    "#!pip install unicodedata\n",
    "#!pip install regex\n",
    "#!pip install torch transformers\n",
    "#!pip install datasets\n",
    "# !pip install SentencePiece\n",
    "# !pip install evaluate\n",
    "# !pip install rouge_scor\n",
    "#!pip install accelerate -U\n",
    "#!pip install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c008c8",
   "metadata": {},
   "source": [
    "#### Getting the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b657b40d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.bbc.com/news/world-us-canada-68269354\n",
      "https://www.bbc.co.uk/sport/american-football/live/ceqj69d5y8yt\n",
      "https://www.bbc.com/news/world-us-canada-68269413\n",
      "https://www.bbc.com/news/entertainment-arts-68238272\n",
      "https://www.bbc.com/news/world-us-canada-68270748\n",
      "https://www.bbc.com/news/world-middle-east-68269957\n",
      "https://www.bbc.com/news/world-africa-68270866\n",
      "https://www.bbc.com/sport/football/68196261\n",
      "https://www.bbc.com/news/world-us-canada-68268817\n",
      "https://www.bbc.com/news/world-asia-68266845\n",
      "https://www.bbc.com/news/world-asia-68262751\n",
      "https://www.bbc.com/news/world-asia-68266845\n",
      "https://www.bbc.com/news/world-asia-68262751\n",
      "https://www.bbc.com/news/world-68266846\n",
      "https://www.bbc.com/news/world-africa-68255614\n",
      "https://www.bbc.com/news/world-latin-america-68268257\n",
      "https://www.bbc.com/sport/american-football/68201059\n",
      "https://www.bbc.com/sport/american-football/68204783\n",
      "https://www.bbc.com/sport/american-football/68250146\n",
      "https://www.bbc.com/sport/american-football/68204790\n",
      "https://www.bbc.com/sport/american-football/68207543\n",
      "https://www.bbc.com/news/world-us-canada-68248168\n",
      "https://www.bbc.com/news/world_radio_and_tv\n",
      "https://www.bbc.com/sounds/play/live:bbc_world_service\n",
      "https://www.bbc.com/news/world-us-canada-68256940\n",
      "https://www.bbc.com/news/world-middle-east-68255843\n",
      "https://www.bbc.com/news/business-68249043\n",
      "https://www.bbc.com/news/world-europe-68255302\n",
      "https://www.bbc.com/news/world-asia-china-68224750\n",
      "https://www.bbc.com/news/world-65432059\n",
      "https://www.bbc.com/news/business-16452878\n",
      "https://www.bbc.com/news/entertainment-arts-68267458\n",
      "https://www.bbc.com/news/world-us-canada-68269820\n",
      "https://www.bbc.com/news/10462520\n",
      "https://www.bbc.com/news/entertainment-arts-68261616\n",
      "https://www.bbc.com/news/world-latin-america-68266339\n",
      "https://www.bbc.com/news/world-68264751\n",
      "https://www.bbc.com/news/world-us-canada-68255422\n",
      "https://www.bbc.com/news/world-europe-68255879\n",
      "https://www.bbc.com/news/world-middle-east-68264363\n",
      "https://www.bbc.com/news/world-asia-68261709\n",
      "https://www.bbc.com/news/world-us-canada-68269413\n",
      "https://www.bbc.com/news/world-us-canada-68270748\n",
      "https://www.bbc.com/news/world-us-canada-68269354\n",
      "https://www.bbc.com/news/world-us-canada-68248168\n",
      "https://www.bbc.com/news/world-us-canada-68268817\n",
      "https://www.bbc.com/news/uk-england-hampshire-68268560\n",
      "https://www.bbc.com/news/world-africa-68270866\n",
      "https://www.bbc.com/news/world-asia-68266845\n",
      "https://www.bbc.com/news/uk-68264821\n",
      "https://www.bbc.com/news/world-asia-68262751\n",
      "https://www.bbc.com/future/article/20240209-the-lost-art-of-the-death-mask\n",
      "https://www.bbc.com/worklife/article/20240208-lunar-new-year-advertising-in-asian-markets\n",
      "https://www.bbc.com/travel/article/20240211-six-expert-picks-for-places-to-eat-and-drink-in-san-franciscos-chinatown\n",
      "https://www.bbc.com/culture/article/20240208-inside-the-homes-that-whisper-rather-than-scream-luxury\n",
      "https://www.bbc.com/future/article/20230913-should-we-be-worried-about-older-politicians\n",
      "https://www.bbc.com/worklife/article/20240207-big-tech-layoffs-perks-cuts\n",
      "https://www.bbc.com/travel/article/20240209-a-chinese-dumpling-with-an-unexpected-twist\n",
      "https://www.bbc.com/sport/football/68270263\n",
      "https://www.bbc.com/sport/football/68269805\n",
      "https://www.bbc.com/sport/football/68270475\n",
      "https://www.bbc.com/sport/rugby-union/68269843\n",
      "https://www.bbc.com/sport/football/68269251\n",
      "https://www.bbc.com/sport/football/68268572\n",
      "https://www.bbc.com/sport/football/68269657\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "\n",
    "url = \"https://www.bbc.co.uk/news\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request is successful\n",
    "if response.status_code == 200:\n",
    "    # get content\n",
    "    soup = bs(response.text, \"html.parser\")\n",
    "\n",
    "    # find all anchor elements with the \"gs-c-promo-heading\" class\n",
    "    article_links = soup.find_all(\"a\", class_=\"gs-c-promo-heading\")\n",
    "\n",
    "    # Create list to store the article URLs\n",
    "    article_urls = []\n",
    "\n",
    "    # Extract article URLs from the href attribute and add to the list\n",
    "    for link in article_links:\n",
    "        article_url = \"https://www.bbc.com\" + link.get(\"href\")\n",
    "        article_urls.append(article_url)\n",
    "\n",
    "    # Clean up URLs with duplicate \"https://www.bbc.com\"\n",
    "    cleaned_urls = []\n",
    "    for url in article_urls:\n",
    "        # Check if \"https\" appears twice in the URL\n",
    "        if url.count(\"https\") == 2:\n",
    "            # Remove the first occurrence of \"https://www.bbc.com\"\n",
    "            cleaned_url = url.replace(\"https://www.bbc.com\", \"\", 1)\n",
    "            cleaned_urls.append(cleaned_url)\n",
    "        else:\n",
    "            cleaned_urls.append(url)\n",
    "\n",
    "    # Print the cleaned list of article URLs\n",
    "    for url in cleaned_urls:\n",
    "        print(url)\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve the page.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d1aed0",
   "metadata": {},
   "source": [
    "#### Extracting Article Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "913cd83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to extract information from a URL\n",
    "def extract_info(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "\n",
    "    # Extracted information\n",
    "    title = article.title\n",
    "    text = article.text\n",
    "    authors = article.authors if article.authors else [\"Author not found\"]\n",
    "\n",
    "    # Generate summary\n",
    "    article.nlp()\n",
    "    summary = article.summary\n",
    "\n",
    "    return {\n",
    "        \"URL\": url,\n",
    "        \"Title\": title,\n",
    "        \"Authors\": authors,\n",
    "        \"Text\": text,\n",
    "        \"TF_IDF_Summary\": summary\n",
    "    }\n",
    "\n",
    "# Extract information for each cleaned URL in the list\n",
    "results = [extract_info(url) for url in cleaned_urls]\n",
    "\n",
    "# Create a Pandas DataFrame from the results\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Get the current date in the format YYYY-MM-DD\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# change the path to save the files for organization\n",
    "directory_path = r\"C:\\Users\\jessi\\Desktop\\Projects Personal\\bbc_article_extracts\"\n",
    "\n",
    "# Combine the directory path with the CSV filename\n",
    "csv_filename = f\"{directory_path}\\\\bbc_articles_info_{current_date}.csv\"\n",
    "\n",
    "# Export the DataFrame to a CSV file with the updated path\n",
    "df.to_csv(csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a81088",
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE THIS TO CHECK DF IN EXCEL\n",
    "# Get the current date in the format YYYY-MM-DD\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Export the DataFrame to a CSV file with the date in the name\n",
    "csv_filename = f\"bbc_articles_info_testing_{current_date}.csv\"\n",
    "df.to_csv(csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b427537",
   "metadata": {},
   "source": [
    "#### Cleaning the Data/Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c0a368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding contractions\n",
    "\n",
    "import contractions\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "# Function to expand contractions in a given text\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "# Apply the function to the 'Text' column and create a new column for cleaned text\n",
    "df['clean_article_text'] = df['Text'].apply(expand_contractions)\n",
    "\n",
    "\n",
    "# Removing unicode characters\n",
    "def remove_unicode(text):\n",
    "    return ''.join(char for char in unicodedata.normalize('NFD', text) if unicodedata.category(char) != 'Mn')\n",
    "\n",
    "# Apply this to the column we just made (will be keeping clean text/updates in one column)\n",
    "df['clean_article_text'] = df['clean_article_text'].apply(remove_unicode)\n",
    "\n",
    "\n",
    "# Converting to lower case\n",
    "df['clean_article_text'] = df['clean_article_text'].str.lower()\n",
    "\n",
    "# Removing special characters and punctuation\n",
    "def remove_special_characters(text):\n",
    "    # Using regex to remove non-alphanumeric characters\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "# Apply the function to the 'clean_article_text' column and update it\n",
    "df['clean_article_text'] = df['clean_article_text'].apply(remove_special_characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357e0484",
   "metadata": {},
   "outputs": [],
   "source": [
    "## up until here, the df looks okay. need to tokenize, remove stop words, then lemmatize. the below messes up the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a63629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76db52ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # using NLTK to do further cleaning\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# # Tokenization\n",
    "\n",
    "# # Download punkt tokenizer\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# # convert the column into a string (or else below doesn't work)\n",
    "# df['clean_article_text'] = df['clean_article_text'].astype(str)\n",
    "\n",
    "# # Apply tokenization to column in df\n",
    "# #df['clean_article_text'] = df['clean_article_text'].apply(word_tokenize)\n",
    "\n",
    "# # Stop word removal\n",
    "\n",
    "# # Download NLTK stop words data\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# # Function to remove stop words\n",
    "# def remove_stopwords(clean_article_text):\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     words = nltk.word_tokenize(clean_article_text)\n",
    "#     filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "#     return ' '.join(filtered_words)\n",
    "\n",
    "# # convert column to string again (it changes after the above)\n",
    "# df['clean_article_text']= df['clean_article_text'].apply(str)\n",
    "\n",
    "# # Apply stop words removal to column in df\n",
    "# df['clean_article_text'] = df['clean_article_text'].apply(remove_stopwords)\n",
    "\n",
    "# # Lemmatization\n",
    "\n",
    "# # Download WordNet lemmatizer\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# # Initialize the lemmatizer\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# # Function to perform lemmatization\n",
    "# def lemmatize_text(clean_article_text):\n",
    "#     #words = nltk.word_tokenize(clean_article_text)\n",
    "#     lemmatized_words = [lemmatizer.lemmatize(filtered_words) for filtered_words in words]\n",
    "#     return ' '.join(lemmatized_words)\n",
    "\n",
    "# # convert column to string again (it changes after the above)\n",
    "# df['clean_article_text']= df['clean_article_text'].apply(str)\n",
    "\n",
    "# # Apply lemmatization to column in df\n",
    "# df['clean_article_text'] = df['clean_article_text'].apply(lemmatize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a2362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#T5 below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df5e4e92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jessi\\anaconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\jessi\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\jessi\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\jessi\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\jessi\\anaconda3\\Lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "#from transformers import AutoTokenizer, AutoModelForSeq2SeqLM  # Updated import\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained('t5-base')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained('t5-base', return_dict=True)  # Updated model initialization\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "\n",
    "\n",
    "#setting up text to be handled by T5\n",
    "\n",
    "# Create an empty column for the T5 summaries and then save\n",
    "df['T5_Summary'] = ''\n",
    "\n",
    "# Loop through each article and generate summary\n",
    "for index, row in df.iterrows():\n",
    "    text = row['Text']\n",
    "    \n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors='pt', max_length=512, truncation=True)\n",
    "    output = model.generate(inputs, min_length=80, max_length=100)\n",
    "    summary = tokenizer.decode(output[0])\n",
    "    \n",
    "    df.at[index, 'T5_Summary'] = summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20900d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import evaluate\n",
    "# import sentencepiece\n",
    "\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# bbc_data = load_dataset('gopalkalpande/bbc-news-summary', split='train')\n",
    "\n",
    "# full_data = bbc_data.train_test_split(test_size=0.2,shuffle=True)\n",
    "# train_data = full_data['train']\n",
    "# valid_data = full_data['test']\n",
    "\n",
    "# print(train_data)\n",
    "# print(valid_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7766bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import T5Tokenizer\n",
    "\n",
    "# tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "\n",
    "# def preprocess_function(examples):\n",
    "#     inputs = [f\"summarize: {article}\" for article in examples['Articles']]\n",
    "#     model_inputs = tokenizer(\n",
    "#         inputs,\n",
    "#         max_length=512,\n",
    "#         truncation=True,\n",
    "#         padding='max_length'\n",
    "#     )\n",
    " \n",
    "#     # Set up the tokenizer for targets\n",
    "#     targets = [summary for summary in examples['Summaries']]\n",
    "#     with tokenizer.as_target_tokenizer():\n",
    "#         labels = tokenizer(\n",
    "#             targets,\n",
    "#             max_length=512,\n",
    "#             truncation=True,\n",
    "#             padding='max_length'\n",
    "#         )\n",
    " \n",
    "#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "#     return model_inputs\n",
    "\n",
    "# # Apply the function to the whole dataset\n",
    "# tokenized_train = train_data.map(\n",
    "#     preprocess_function,\n",
    "#     batched=True,\n",
    "#     num_proc=3\n",
    "# )\n",
    "# tokenized_valid = valid_data.map(\n",
    "#     preprocess_function,\n",
    "#     batched=True,\n",
    "#     num_proc=3\n",
    "# )\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313bf741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from datasets import load_dataset\n",
    "# from transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "# # Load the dataset\n",
    "# bbc_data = load_dataset('gopalkalpande/bbc-news-summary', split='train')\n",
    "\n",
    "# # Split the dataset into train and test sets\n",
    "# full_data = bbc_data.train_test_split(test_size=0.2, shuffle=True)\n",
    "# train_data = full_data['train']\n",
    "# test_data = full_data['test']\n",
    "\n",
    "# # Initialize the tokenizer in the global scope\n",
    "# tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "# def preprocess_function(examples, tokenizer):\n",
    "#     inputs = [f\"summarize: {article}\" for article in examples['Articles']]\n",
    "#     model_inputs = tokenizer(\n",
    "#         inputs,\n",
    "#         max_length=512,\n",
    "#         truncation=True,\n",
    "#         padding='max_length'\n",
    "#     )\n",
    " \n",
    "#     # Set up the tokenizer for targets\n",
    "#     targets = [summary for summary in examples['Summaries']]\n",
    "#     with tokenizer.as_target_tokenizer():\n",
    "#         labels = tokenizer(\n",
    "#             targets,\n",
    "#             max_length=512,\n",
    "#             truncation=True,\n",
    "#             padding='max_length'\n",
    "#         )\n",
    " \n",
    "#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "#     return model_inputs\n",
    "\n",
    "# # Apply the function to the whole dataset\n",
    "# tokenized_train = train_data.map(\n",
    "#     preprocess_function,\n",
    "#     batched=True,\n",
    "#     num_proc=3,\n",
    "#     fn_kwargs={'tokenizer': tokenizer}\n",
    "# )\n",
    "# tokenized_test = test_data.map(\n",
    "#     preprocess_function,\n",
    "#     batched=True,\n",
    "#     num_proc=3,\n",
    "#     fn_kwargs={'tokenizer': tokenizer}\n",
    "# )\n",
    "\n",
    "# # Fine-tune the model\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     per_device_train_batch_size=8,\n",
    "#     per_device_eval_batch_size=8,\n",
    "#     predict_with_generate=True,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     save_steps=500,\n",
    "#     eval_steps=500,\n",
    "#     logging_steps=500,\n",
    "#     logging_dir='./logs',\n",
    "#     do_train=True,\n",
    "#     do_eval=True,\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"rouge\",\n",
    "#     greater_is_better=True,\n",
    "#     num_train_epochs=3,\n",
    "#     learning_rate=1e-5,\n",
    "# )\n",
    "\n",
    "# trainer = Seq2SeqTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     data_collator = lambda data: {\n",
    "#     'input_ids': torch.stack([item['input_ids'] for item in data]),\n",
    "#     'attention_mask': torch.stack([item['attention_mask'] for item in data]),\n",
    "#     'labels': torch.stack([item['labels'] for item in data])},\n",
    "#     train_dataset=tokenized_train,\n",
    "#     eval_dataset=tokenized_test,\n",
    "#     tokenizer=tokenizer,\n",
    "# )\n",
    "\n",
    "\n",
    "# # Train the model\n",
    "# trainer.train()\n",
    "\n",
    "# # Evaluate the model using ROUGE score\n",
    "# rouge = Rouge()\n",
    "# eval_results = trainer.predict(tokenized_test)\n",
    "# references = [\" \".join(ex['Summaries']) for ex in test_data]\n",
    "# predictions = [\" \".join(rouge_output[0][\"summary_text\"].split()) for rouge_output in eval_results.predictions]\n",
    "# rouge_scores = rouge.get_scores(predictions, references, avg=True)\n",
    "\n",
    "# print(\"ROUGE Scores:\", rouge_scores)\n",
    "\n",
    "# # Use the fine-tuned model to create summaries for your own data\n",
    "# # Assuming df is your DataFrame with the 'Text' column\n",
    "# df['FineTuned_Summary'] = ''\n",
    "\n",
    "# for index, row in df.iterrows():\n",
    "#     text = row['Text']\n",
    "    \n",
    "#     inputs = tokenizer.encode(\"summarize: \" + text, return_tensors='pt', max_length=512, truncation=True)\n",
    "#     output = model.generate(inputs, min_length=80, max_length=100)\n",
    "#     summary = tokenizer.decode(output[0])\n",
    "    \n",
    "#     df.at[index, 'FineTuned_Summary'] = summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eafb3544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jessi\\anaconda3\\Lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map (num_proc=3): 100%|█████████████████████████████████████████████████████| 1779/1779 [00:35<00:00, 50.15 examples/s]\n",
      "Map (num_proc=3): 100%|███████████████████████████████████████████████████████| 445/445 [00:25<00:00, 17.58 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222,903,552 total parameters.\n",
      "222,903,552 training parameters.\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "# Load the dataset\n",
    "bbc_data = load_dataset('gopalkalpande/bbc-news-summary', split='train')\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "full_data = bbc_data.train_test_split(test_size=0.2, shuffle=True)\n",
    "train_data = full_data['train']\n",
    "test_data = full_data['test']\n",
    "\n",
    "# Initialize the tokenizer in the global scope\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    inputs = [f\"summarize: {article}\" for article in examples['Articles']]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    " \n",
    "    # Set up the tokenizer for targets\n",
    "    targets = [summary for summary in examples['Summaries']]\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding='max_length'\n",
    "        )\n",
    " \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply the function to the whole dataset\n",
    "tokenized_train = train_data.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=3,\n",
    "    fn_kwargs={'tokenizer': tokenizer}\n",
    ")\n",
    "tokenized_test = test_data.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=3,\n",
    "    fn_kwargs={'tokenizer': tokenizer}\n",
    ")\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc67165a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jessi\\AppData\\Local\\Temp\\ipykernel_17968\\1058471435.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n",
      "C:\\Users\\jessi\\anaconda3\\Lib\\site-packages\\datasets\\load.py:752: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/rouge/rouge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "rouge = load_metric(\"rouge\")\n",
    " \n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred.predictions[0], eval_pred.label_ids\n",
    " \n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    " \n",
    "    result = rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True,\n",
    "        rouge_types=[\n",
    "            'rouge1',\n",
    "            'rouge2',\n",
    "            'rougeL'\n",
    "        ]\n",
    "    )\n",
    " \n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    " \n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    Original Trainer may have a memory leak.\n",
    "    This is a workaround to avoid storing too many tensors that are not needed.\n",
    "    \"\"\"\n",
    "    pred_ids = torch.argmax(logits[0], dim=-1)\n",
    "    return pred_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b193389",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 45.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 33\u001b[0m\n\u001b[0;32m      3\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m      4\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults_t5base\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m \n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     24\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     25\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     26\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[0;32m     31\u001b[0m )\n\u001b[1;32m---> 33\u001b[0m history \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1535\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1538\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   1539\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   1540\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   1541\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   1542\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1854\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1851\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   1853\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1854\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1857\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1859\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1860\u001b[0m ):\n\u001b[0;32m   1861\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1862\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2735\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2732\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2735\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs)\n\u001b[0;32m   2737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2738\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2758\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2756\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2757\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2758\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   2759\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2760\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\accelerate\\utils\\operations.py:687\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 687\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\accelerate\\utils\\operations.py:675\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\accelerate\\utils\\operations.py:687\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 687\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\accelerate\\utils\\operations.py:675\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1743\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1740\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[1;32m-> 1743\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[0;32m   1744\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39mdecoder_input_ids,\n\u001b[0;32m   1745\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mdecoder_attention_mask,\n\u001b[0;32m   1746\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39mdecoder_inputs_embeds,\n\u001b[0;32m   1747\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1748\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m   1749\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1750\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mdecoder_head_mask,\n\u001b[0;32m   1751\u001b[0m     cross_attn_head_mask\u001b[38;5;241m=\u001b[39mcross_attn_head_mask,\n\u001b[0;32m   1752\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1753\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1754\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1755\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1756\u001b[0m )\n\u001b[0;32m   1758\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1760\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1110\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1096\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[0;32m   1097\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1107\u001b[0m         output_attentions,\n\u001b[0;32m   1108\u001b[0m     )\n\u001b[0;32m   1109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1110\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m   1111\u001b[0m         hidden_states,\n\u001b[0;32m   1112\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   1113\u001b[0m         position_bias\u001b[38;5;241m=\u001b[39mposition_bias,\n\u001b[0;32m   1114\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1115\u001b[0m         encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[0;32m   1116\u001b[0m         encoder_decoder_position_bias\u001b[38;5;241m=\u001b[39mencoder_decoder_position_bias,\n\u001b[0;32m   1117\u001b[0m         layer_head_mask\u001b[38;5;241m=\u001b[39mlayer_head_mask,\n\u001b[0;32m   1118\u001b[0m         cross_attn_layer_head_mask\u001b[38;5;241m=\u001b[39mcross_attn_layer_head_mask,\n\u001b[0;32m   1119\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m   1120\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1121\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1122\u001b[0m     )\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:724\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    722\u001b[0m     query_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 724\u001b[0m cross_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer[\u001b[38;5;241m1\u001b[39m](\n\u001b[0;32m    725\u001b[0m     hidden_states,\n\u001b[0;32m    726\u001b[0m     key_value_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m    727\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m    728\u001b[0m     position_bias\u001b[38;5;241m=\u001b[39mencoder_decoder_position_bias,\n\u001b[0;32m    729\u001b[0m     layer_head_mask\u001b[38;5;241m=\u001b[39mcross_attn_layer_head_mask,\n\u001b[0;32m    730\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mcross_attn_past_key_value,\n\u001b[0;32m    731\u001b[0m     query_length\u001b[38;5;241m=\u001b[39mquery_length,\n\u001b[0;32m    732\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    733\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    734\u001b[0m )\n\u001b[0;32m    735\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    737\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:635\u001b[0m, in \u001b[0;36mT5LayerCrossAttention.forward\u001b[1;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions)\u001b[0m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    624\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    632\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    633\u001b[0m ):\n\u001b[0;32m    634\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[1;32m--> 635\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEncDecAttention(\n\u001b[0;32m    636\u001b[0m         normed_hidden_states,\n\u001b[0;32m    637\u001b[0m         mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    638\u001b[0m         key_value_states\u001b[38;5;241m=\u001b[39mkey_value_states,\n\u001b[0;32m    639\u001b[0m         position_bias\u001b[38;5;241m=\u001b[39mposition_bias,\n\u001b[0;32m    640\u001b[0m         layer_head_mask\u001b[38;5;241m=\u001b[39mlayer_head_mask,\n\u001b[0;32m    641\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m    642\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    643\u001b[0m         query_length\u001b[38;5;241m=\u001b[39mquery_length,\n\u001b[0;32m    644\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    645\u001b[0m     )\n\u001b[0;32m    646\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    647\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:564\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[1;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    560\u001b[0m scores \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_bias_masked\n\u001b[0;32m    561\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(scores\u001b[38;5;241m.\u001b[39mfloat(), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtype_as(\n\u001b[0;32m    562\u001b[0m     scores\n\u001b[0;32m    563\u001b[0m )  \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n\u001b[1;32m--> 564\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(\n\u001b[0;32m    565\u001b[0m     attn_weights, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining\n\u001b[0;32m    566\u001b[0m )  \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;66;03m# Mask heads if we want to\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layer_head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:1268\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28minput\u001b[39m, p, training)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 45.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='results_t5base',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='results_t5base',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=200,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=2,\n",
    "    report_to='tensorboard',\n",
    "    learning_rate=0.0001,\n",
    "    dataloader_num_workers=2,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=6\n",
    "\n",
    ")\n",
    " \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    " \n",
    "history = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c738afa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# # Assuming you have defined 'model', 'tokenized_train', 'tokenized_test', and 'compute_metrics' somewhere in your code\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='results_t5base',\n",
    "#     num_train_epochs=3,\n",
    "#     per_device_train_batch_size=2,  # Adjust according to available CPU memory\n",
    "#     per_device_eval_batch_size=2,   # Adjust according to available CPU memory\n",
    "#     warmup_steps=500,\n",
    "#     weight_decay=0.01,\n",
    "#     logging_dir='results_t5base',\n",
    "#     logging_steps=10,\n",
    "#     evaluation_strategy='steps',\n",
    "#     eval_steps=200,\n",
    "#     save_strategy='epoch',\n",
    "#     save_total_limit=2,\n",
    "#     report_to='tensorboard',\n",
    "#     learning_rate=0.00005,  # Experiment with different learning rates\n",
    "#     dataloader_num_workers=2,  # Adjust according to your system's capacity\n",
    "#     gradient_accumulation_steps=1,\n",
    "#     fp16=False,\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_train,\n",
    "#     eval_dataset=tokenized_test,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )\n",
    "\n",
    "# history = trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046898c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune t5 on bbc summary dataset\n",
    "# evaluate it, and then run again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bc8d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244c58a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Using Pysummarization\n",
    "# refer to the documentation online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f1939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Using BERT via huggingface transformers (or use T5) check the link\n",
    "#  https://keras.io/examples/nlp/t5_hf_summarization/       this uses T5\n",
    "\n",
    "# https://datagraphi.com/blog/post/2021/9/24/comparing-performance-of-a-modern-nlp-framework-bert-vs-a-classical-approach-tf-idf-for-document-classification-with-simple-and-easy-to-understand-code\n",
    "# ^ this uses BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1164e0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "# train_dataset = dataset['train']\n",
    "# test_dataset = dataset['test']\n",
    "\n",
    "# from transformers import BertTokenizer\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# def tokenize_batch(batch):\n",
    "#     return tokenizer(batch['article'], padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "# train_dataset = train_dataset.map(tokenize_batch, batched=True)\n",
    "# test_dataset = test_dataset.map(tokenize_batch, batched=True)\n",
    "\n",
    "# import torch\n",
    "# from transformers import BertForSeq2Seq\n",
    "\n",
    "# model = BertForSeq2Seq.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Define optimizer and scheduler\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# def compute_metrics(pred):\n",
    "#     # Define your evaluation metric (e.g., BLEU, ROUGE, etc.)\n",
    "#     # For this example, we'll use dummy metrics\n",
    "#     return {\"accuracy\": 0.5}\n",
    "\n",
    "# from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     per_device_train_batch_size=2,\n",
    "#     per_device_eval_batch_size=2,\n",
    "#     predict_with_generate=True,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     save_steps=500,\n",
    "#     eval_steps=500,\n",
    "#     logging_steps=500,\n",
    "#     logging_dir='./logs',\n",
    "#     do_train=True,\n",
    "#     do_eval=True,\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"accuracy\",\n",
    "#     greater_is_better=True,\n",
    "#     num_train_epochs=3,\n",
    "#     learning_rate=1e-5,\n",
    "# )\n",
    "\n",
    "# trainer = Seq2SeqTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]), \n",
    "#                                 'attention_mask': torch.stack([f[1] for f in data]), \n",
    "#                                 'labels': torch.stack([f[2] for f in data])},\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=test_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )\n",
    "\n",
    "# trainer.train()\n",
    "\n",
    "# results = trainer.evaluate()\n",
    "# print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359c8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## bert after trained/tuned on my own data\n",
    "# # Tokenizing the dataframe\n",
    "# def tokenize_articles(text):\n",
    "#     inputs = tokenizer(text, max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "#     return inputs\n",
    "\n",
    "# df['tokenized'] = df['cleaned_article_text'].apply(tokenize_articles)\n",
    "\n",
    "# def generate_summary(input_ids, attention_mask):\n",
    "#     input_ids = input_ids.to(device)\n",
    "#     attention_mask = attention_mask.to(device)\n",
    "    \n",
    "#     # Generate summary\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=150, num_beams=4, length_penalty=2.0, early_stopping=True)\n",
    "    \n",
    "#     summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     return summary\n",
    "\n",
    "# df['generated_summary'] = df['tokenized'].apply(lambda x: generate_summary(x['input_ids'], x['attention_mask']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2137f32d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a40a43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ece97eab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available for PyTorch: True\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "\n",
    "# # Check if GPU is available for PyTorch\n",
    "# is_gpu_available = torch.cuda.is_available()\n",
    "# print(f\"GPU available for PyTorch: {is_gpu_available}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e5993e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
