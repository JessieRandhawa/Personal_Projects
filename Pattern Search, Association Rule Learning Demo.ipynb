{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6f3bd57",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "Using company data, this demo will briefly cover some of the concepts and steps to performing pattern search/association rule analysis. Prior to running the notebook, please have the following packges installed:\n",
    "\n",
    "- Pandas\n",
    "- Numpy\n",
    "- Matplotlib\n",
    "- Mlxtend\n",
    "- PyECLAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b316ff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dd08ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataset\n",
    "\n",
    "df = pd.read_csv(\"C:/\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fe792c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some general info on dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124c39c1",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1af571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the summary stats to better understand data\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ef62ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for null/blank rows\n",
    "df.isnull().values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff183d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What product has most shipments\n",
    "col_sum = df.sum()\n",
    "highest_col_sum = col_sum.idxmax()\n",
    "highest_sum=col_sum[highest_col_sum]\n",
    "\n",
    "print(\"Most shipped product:\", highest_col_sum)\n",
    "print(\"Amount shipped:\", highest_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b5b6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.bar(col_sum.index,col_sum.values)\n",
    "plt.xlabel('Products')\n",
    "plt.ylabel('Count of obs')\n",
    "plt.title('Total Count')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c78691",
   "metadata": {},
   "source": [
    "### Preprocessing/Formatting Dataset\n",
    "\n",
    "Before doing any kind of metrics/evaluation, we need to format the dataset so that it can be used properly.\n",
    "Completing the steps below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc69b46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy() # make a copy of the dataset\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55012394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column for transaction ID to be at beginning of DF\n",
    "df1 = df1.assign(Transaction+ID = range(1, len(df1) +1))\n",
    "df1 = df1.set_index(\"Transaction_ID\")\n",
    "\n",
    "#Need the rows to show as True/False instead of 0 or 1 (needed for algorithms later)\n",
    "\n",
    "df1 = df1.fillna(0).astype(bool)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f71731c",
   "metadata": {},
   "source": [
    "### Evaluation Metrics & Calculations\n",
    "\n",
    "The metrics used to analyze pattern search/association rules are listed below. Each metric measures the level of interestingness/importance of the rule in question. The below is some info about each metric:\n",
    "\n",
    " - Support: Measures the frequency of occurrence of a rule in the dataset\n",
    " - Confidence: Represents the conditional probability of the consequent (then) given the antecent (if), telling us how reliable the rule is\n",
    " - Lift: Measures the strength of association between the antecedent and consequent by comparing the observed support with the expected support if the items were independent\n",
    " - Leverage: Quantifies the difference between the observed support and expected support if the items were independent, indicating the co-occurrence of the antecedent and consequent\n",
    " - Conviction: Measures the degree of dependency between the antecedent and consequent, indicating how much the consequent relies on the antecedent for its occurence\n",
    " \n",
    "Note, support and confidence are the most commonly used metrics for evaluating rules. Lift is a good metric to use to determine if a rule should be pruned (removed) or not.\n",
    "\n",
    "For more info, you should check out the wiki page on association rule learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d29890a",
   "metadata": {},
   "source": [
    "#### Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3960c9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating support for individual products\n",
    "support = df1.mean().sort_values(ascending = False)\n",
    "support.head(10) # give me the top 10 products with highest support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f73097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can also calculate support for bundles\n",
    "\n",
    "df_bundles = df1.copy()\n",
    "df_bundles['product1 and p2'] = np.logical_and(df_bundles['product1'],df_bundles['p2'])\n",
    "df_bundles['product1 and p3'] = np.logical_and(df_bundles['product1'],df_bundles['p3'])\n",
    "df_bundles['p2 and p4'] = np.logical_and(df_bundles['p2'],df_bundles['p4'])\n",
    "\n",
    "# show the support for the new bundles\n",
    "new_bundles = ['p1 and p2', 'p1 and p3', 'p2 and p4']\n",
    "support = df_bundles.mean().sort_values(ascending = False) # adding bundles to main df\n",
    "\n",
    "print(support[new_bundles].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe3c655",
   "metadata": {},
   "source": [
    "#### Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2771d18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confidence of p1 and p2\n",
    "print(support['p1 and p2']/support['p1'])\n",
    "\n",
    "# for p1 and p3\n",
    "print(support['p1 and p3']/support['p1'])\n",
    "\n",
    "# for p2 and p4\n",
    "print(support['p2 and p4']/support['p2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b00134",
   "metadata": {},
   "source": [
    "#### Lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06aa0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lift of p1 and p2\n",
    "print(support['p1 and p2']/(support['p1']*support['p2']))\n",
    "\n",
    "# for p1 and p3\n",
    "print(support['p1 and p3']/(support['p1']*support['p3']))\n",
    "\n",
    "# for p2 and p4\n",
    "print(support['p2 and p4']/(support['p2']*support['p4']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7edd5a9",
   "metadata": {},
   "source": [
    "#### leverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1903a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leverage of p1 and p2\n",
    "print(support['p1 and p2'] - (support['p1']*support['p2']))\n",
    "\n",
    "# for p1 and p3\n",
    "print(support['p1 and p3'] - (support['p1']*support['p3']))\n",
    "\n",
    "# for p2 and p4\n",
    "print(support['p2 and p4'] - (support['p2']*support['p4']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84520f19",
   "metadata": {},
   "source": [
    "#### Conviction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35b32b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conviction of p1 and p2\n",
    "print(support['p1']*(1-support['p2']) / (support['p1'] - support['p1 and p2']))\n",
    "\n",
    "#for p1 and p3\n",
    "print(support['p1']*(1-support['p3']) / (support['p1'] - support['p1 and p3']))\n",
    "\n",
    "#for p2 and p4\n",
    "print(support['p2']*(1-support['p4']) / (support['p2'] - support['p2 and p4']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ee4783",
   "metadata": {},
   "source": [
    "### Algorithms\n",
    "\n",
    "the below section goes over some examples of the different algorithms used in pattern search. Each one approaches the dataset differently. A brief explanation of each one:\n",
    "\n",
    " - Apriori: looks at items that frequently appear together and generates rules based on how often they occur together, using support + confidence thresholds to do so.\n",
    " - ECLAT: Uses a depth-first search strategy to discover frequent itemsets and generate rules by exploiting the vertical data format.\n",
    " - F-P Growth: similar to Apriori, but looks at items individually, organizing them into a structure called F-P Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abe2b92",
   "metadata": {},
   "source": [
    "#### Apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f31c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the packages needed\n",
    "\n",
    "from mlxtend.frequent_patterns import apriori, association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97a9709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the apriori method\n",
    "apriori = apriori(df_bundles, min_support = 0.01, use_colnames=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64e9009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make rules\n",
    "rules = association_rules(apriori, metric=\"confidence\", min_threshold=0.5)\n",
    "rules.sort_values(by = 'support', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76439bda",
   "metadata": {},
   "source": [
    "# INSERT SOME INFO ON THE RESULTS ABOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c46f1e6",
   "metadata": {},
   "source": [
    "#### ECLAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46a7a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed packages\n",
    "from pyECLAT import ECLAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb86e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use this, must reformat the df so that it fits specific criteria\n",
    "# you can get more info on this by creating a cell containing \"help(ECLAT)\" and running it\n",
    "\n",
    "# will use our original df to start\n",
    "df2 = df.copy()\n",
    "\n",
    "product_names = list(df2.columns[1:]) #creating list of product names\n",
    "new_df = pd.DataFrame(columns = product_names) #create new df using cols above\n",
    "\n",
    "for column in df1.columns:\n",
    "    transaction_data = [] #these are transation IDs/rows\n",
    "    \n",
    "    for index,value in df1[column].items():\n",
    "        if value == 1:\n",
    "            transaction_data.append(column)\n",
    "        else:\n",
    "            transaction_data.append('')\n",
    "    \n",
    "    new_df[column] = transaction_data\n",
    "    \n",
    "new_df.reset_index(drop=True, inplace=True) #reset index\n",
    "\n",
    "new_df.columns = range(len(new_df.columns)) # change col names from product name to numbers instead\n",
    "\n",
    "new_df = new_df.replace('', np.nan) #replace blanks with NaNs instead\n",
    "\n",
    "#This is the new df\n",
    "new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e23954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ECLAT algorithm\n",
    "eclat = ECLAT(data = new_df, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47053df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the algorithm on our data\n",
    "rule_indices, rule_supports = eclat.fit(min_support = 0.03, min_combination = 2, max_combination = 4)\n",
    "\n",
    "#Limiting this to a max of 4x4 combinations and support must be >= 0.03, or this takes a very long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86791b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the rules\n",
    "\n",
    "eclat_result = pd.DataFrame(rule_supports.items(), columns=['Item','Support'])\n",
    "eclat_result.sort_values(by=['Support'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3283e3e1",
   "metadata": {},
   "source": [
    "#### F-P Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e88fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating F-P growth method\n",
    "from mlxtend.frequent_patterns import fpmax, fpgrowth, association_rules\n",
    "\n",
    "fpgrowth = fpgrowth(df_bundles, min_support = 0.01, use_colnames = True)\n",
    "#fpgrowth.sort_values(by = 'support', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281ddef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make rules\n",
    "rulesfp = association_rules(fpgrowth, metric=\"lift\", min_threshold = 0.5)\n",
    "rulesfp = rulesfp.sort_values(['confidence','lift'], ascending = [False, False])\n",
    "rulesfp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18671513",
   "metadata": {},
   "source": [
    "# put info on interpreting the results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
